{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "increasing-basement",
   "metadata": {},
   "source": [
    "# End-to-End Notebook GSK & Broad Institute\n",
    "\n",
    "This notebook is designed to enable to the following steps:\n",
    "\n",
    "1. Load pre-trained virtual stain model and generate predictions\n",
    "2. Generate instance masks (nuclei or cytoplasm) via CellPose for virtual stain predictions\n",
    "3. Normalise predictions to 8bit (cellprofiler requirement)\n",
    "4. Run Cellprofiler pipeline and save features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "productive-specialist",
   "metadata": {},
   "source": [
    "\n",
    "## Load libraries and helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "seven-moisture",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'slurm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mslurm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msbatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m submit_array\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mslurm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommands\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inference_cellpose\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcellpose\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m read_yaml, write_yaml\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'slurm'"
     ]
    }
   ],
   "source": [
    "\n",
    "import os \n",
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from slurm.sbatch import submit_array\n",
    "from slurm.commands import inference_cellpose\n",
    "from cellpose.io import read_yaml, write_yaml\n",
    "root_dir = \"../../../\"\n",
    "sys.path.append(root_dir)\n",
    "\n",
    "def dict_to_str(_dict: dict):\n",
    "    \"\"\"\n",
    "    Change from dict to string conversion.\n",
    "\n",
    "    :param _dict:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s = ''\n",
    "    for k, v in _dict.items():\n",
    "        s = s + k + ' ' + v + ' '\n",
    "    return s\n",
    "\n",
    "\n",
    "def test_pix2pixHD(py_file: str, arg_dict: str):\n",
    "    \"\"\"\n",
    "    Generate slurm command to train Pix2PixHD.\n",
    "\n",
    "    :param py_file:\n",
    "    :param arg_dict:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    command = f'python {py_file} {dict_to_str(arg_dict)}'\n",
    "    return command\n",
    "\n",
    "def normalize(arr):\n",
    "    \"\"\"\n",
    "    8bit normalisation.\n",
    "    Despite full range being 65535 we found when we randomly sampled\n",
    "    from our DAPI stain the average of 10,000 samples over three random \n",
    "    seeds for min and max was 85.0 and 12573 so we perform minmax norm \n",
    "    with these values. This is only completed as Cellprofiler pipeline\n",
    "    requires images to be 8bit intergers.\n",
    "\n",
    "    :param arr:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    scale_arr = (arr - 85.0) / (12573.0 - 85.0)\n",
    "    scale_arr = scale_arr * 255\n",
    "    scale_arr = scale_arr.astype(np.uint8)\n",
    "    return scale_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generic-force",
   "metadata": {},
   "source": [
    "## Load pre-trained virtual stain model and generate predictions\n",
    "\n",
    "This notebook is used to run inference on a pre-trained Pix2PixHD GAN model\n",
    "\n",
    "1. Example bright-field \"AssayPlate_PerkinElmer_CellCarrierUltra_P24_t0024F005L01A01Z01C01.tif\". Predictions are stored in seperate folders based on P24. </c>\n",
    "    Leads to 387 folders. Each folder is then paralysed and run concurrently through the next stages .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "minute-involvement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Jobs 1\n",
      "Submitted batch job 26801\n",
      "\n"
     ]
    }
   ],
   "source": [
    "repo_path = os.path.join(\n",
    "    '/hpc/projects/upt/samuel_tonks_experimental_space/repos/gskgithub/GSK-Broad/pix2pixHD/'\n",
    "    )\n",
    "\n",
    "py_file = os.path.join(\n",
    "        repo_path,\n",
    "        'test_gskbroad.py'\n",
    "    )\n",
    "\n",
    "conda_path = os.path.join(\n",
    "    '/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/'\n",
    "    )\n",
    "\n",
    "model = 'nuclei' # nuclei or cyto\n",
    "\n",
    "model_dir = os.path.join(\n",
    "    repo_path,\n",
    "    'weights',\n",
    "    model\n",
    "    )\n",
    "\n",
    "output_folder_name = 'GSK-Broad'\n",
    "\n",
    "input_dir = os.path.join(\n",
    "    '/hpc/scratch/rdip1/smt29021/XAI/ELN128360/BFcellhealth_20230322_113718/AssayPlate_PerkinElmer_CellCarrierUltra/'\n",
    "    )\n",
    "\n",
    "output_dir = os.path.join(\n",
    "    '/hpc/projects/upt/samuel_tonks_experimental_space/repos/gskgithub/'\n",
    "    ,output_folder_name\n",
    "    )\n",
    "\n",
    "\n",
    "# Pix2PixHD GAN params\n",
    "command_list = []\n",
    "arg_dict = {}\n",
    "# To be changed\n",
    "arg_dict['--checkpoints_dir'] = model_dir\n",
    "arg_dict['--dataroot'] = input_dir\n",
    "arg_dict['--name'] = 'gsk-broad'\n",
    "\n",
    "# GSK-Broad data is 1000x1000\n",
    "arg_dict['--loadSize'] = '1000'\n",
    "arg_dict['--output_reshape'] = '1000'  \n",
    "arg_dict['--resize_or_crop'] = 'none'\n",
    "arg_dict['--how_many'] = '3' #  total number of bright-field\n",
    "\n",
    "# Not to be changed\n",
    "arg_dict['--data_type'] = '16'\n",
    "arg_dict['--label_nc'] = '0'\n",
    "arg_dict['--input_nc'] = '1'\n",
    "arg_dict['--output_nc'] = '1'\n",
    "arg_dict['--no_instance'] = ''\n",
    "arg_dict['--gpu_ids'] = '0'\n",
    "arg_dict['--norm'] = 'instance'\n",
    "arg_dict['--results_dir'] = os.path.join(output_dir)\n",
    "arg_dict['--which_epoch'] = 'latest' \n",
    "\n",
    "if not os.path.exists(os.path.join(output_dir,f\"test_{'latest'}\")):\n",
    "    os.makedirs(os.path.join(output_dir,f\"test_{'latest'}\"))\n",
    "\n",
    "command = test_pix2pixHD(py_file,arg_dict)\n",
    "command_list.append(command)\n",
    "    \n",
    "print(\"Number of Jobs {}\".format(len(command_list)))\n",
    "\n",
    "### Job Settings ###\n",
    "\n",
    "job_name = 'VNuclei_PHD_Inference'\n",
    "\n",
    "node_setting = ''\n",
    "node_setting = node_setting+' --job-name={}'.format(job_name)\n",
    "node_setting = node_setting+' --time=4:00:00'\n",
    "node_setting = node_setting+' --nodes=1'\n",
    "node_setting = node_setting+' --partition=gpu'\n",
    "node_setting = node_setting+' --gres=gpu:a6000:1'\n",
    "node_setting = node_setting+' --ntasks-per-node=1'\n",
    "node_setting = node_setting+' --output=./slurm_outs/\"slurm-%A_%a.out\"'\n",
    "node_setting = node_setting[1:]\n",
    "\n",
    "os.makedirs('./slurm_outs', exist_ok=True)\n",
    " \n",
    "jobid = submit_array(root_dir, command_list, node_setting, job_name, repo_path,conda_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-retailer",
   "metadata": {},
   "source": [
    "## Generate instance masks (nuclei or cytoplasm) via CellPose for virtual stain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-kingston",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPC Settings\n",
    "# Choose HPC\n",
    "hpc = \"US\" # Should be \"UK\" or \"US\"\n",
    "\n",
    "# Set HPC related variables\n",
    "if hpc == \"UK\":\n",
    "    hpc_project = \"/hpc/projects/stv/bioimaging_analytics\"\n",
    "    hpc_cpu = \" --partition=stv-cpu\"\n",
    "    hpc_gpu = \" --partition=stv-gpu\"\n",
    "\n",
    "elif hpc == \"US\":\n",
    "    hpc_project = \"/hpc/projects/upt/samuel_tonks_experimental_space/experiments/\"\n",
    "    hpc_cpu = \" --partition=up-cpu\"\n",
    "    hpc_gpu = \" --partition=up-gpu\"\n",
    "\n",
    "else:\n",
    "    print(f\"HPC setting does not exists.\")\n",
    "\n",
    "# Define Common Variables\n",
    "\n",
    "# Set dataset name, should be consistent with folder structure\n",
    "dataset_name = 'GSKBroad'\n",
    "conda_path = \"/path/to/cellpose/env\"\n",
    "\n",
    "# Populate shared variables\n",
    "project_dir = os.path.join(hpc_project, \"GSK-Broad\")\n",
    "config_dir = os.path.join(root_dir, \"configs\")\n",
    "tmp_dir = os.path.join(root_dir, \"tmp\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GSK-Broad\n",
    "\n",
    "# Identify all images\n",
    "gsk_broad = sorted([p for p in Path(f'{output_dir}/').glob(f'**/*_virtualstain.tiff')])\n",
    "\n",
    "# Identify Subfolders\n",
    "folders = set([str(i).split('/')[-2] for i in gsk_broad])\n",
    "\n",
    "# Iterate input folder and build cellpose command for cell and nuclei segmentation\n",
    "command_list = []\n",
    "for f in list(folders):\n",
    "    dataset_name = f\n",
    "    path = os.path.join(\n",
    "        gsk_broad_path,\n",
    "        f\n",
    "    )\n",
    "    chosen_path = path\n",
    "    yml_dir = os.path.join(tmp_dir, \"inference_cellpose\", dataset_name)\n",
    "    py_path = os.path.join(root_dir, \"cellpose\", \"inference_cellpose.py\")\n",
    "    for root, dirs, files in os.walk(chosen_path):\n",
    "        if not dirs:\n",
    "            cell_list = []\n",
    "            nuclei_list = []\n",
    "            for file in files:\n",
    "                # Build Input: Output pair\n",
    "                input_file = os.path.join(chosen_path, file)\n",
    "                output_file = os.path.join(chosen_path, file[:-4]+'_mask.tiff')\n",
    "                io_dict = {\"input\": input_file, \"output\": output_file}\n",
    "                # Append to corresponding list\n",
    "                if model == 'cyto':\n",
    "                    cell_list.append(io_dict)\n",
    "                elif model == 'nuclei':\n",
    "                    nuclei_list.append(io_dict)\n",
    "                else:\n",
    "                    continue\n",
    "    # Build cellpose cell segmentation commands\n",
    "    if model == 'cyto':\n",
    "        cell_yml = os.path.join(yml_dir,dataset_name +\"_Cell.yml\")\n",
    "        args_dict = {}\n",
    "        args_dict[\"--chan\"] = \"0\"\n",
    "        args_dict[\"--save_tif\"] = \"\"\n",
    "        args_dict[\"--use_gpu\"] = \"\"\n",
    "        args_dict[\"--diameter\"] = \"0.\"\n",
    "        args_dict[\"--file_yaml\"] = cell_yml\n",
    "        args_dict[\"--pretrained_model\"] = \"cyto\"\n",
    "\n",
    "        command = inference_cellpose(py_path, args_dict)\n",
    "        command_list.append(command)\n",
    "        write_yaml(cell_yml, cell_list)\n",
    "\n",
    "    # Build cellpose nuclei segmentation commands\n",
    "    elif model == 'nuclei':\n",
    "        nuclei_yml = os.path.join(yml_dir, dataset_name+\"_Nuclei.yml\")\n",
    "        args_dict = {}\n",
    "        args_dict[\"--chan\"] = \"0\"\n",
    "        args_dict[\"--save_tif\"] = \"\"\n",
    "        args_dict[\"--use_gpu\"] = \"\"\n",
    "        args_dict[\"--diameter\"] = \"0.\"\n",
    "        args_dict[\"--file_yaml\"] = nuclei_yml\n",
    "        args_dict[\"--pretrained_model\"] = \"nuclei\"\n",
    "\n",
    "        command = inference_cellpose(py_path, args_dict)\n",
    "        command_list.append(command)\n",
    "        write_yaml(nuclei_yml, nuclei_list)\n",
    "\n",
    "print(command)\n",
    "print(f\"Number of Jobs: {len(command_list)}\")\n",
    "\n",
    "# Job array settings\n",
    "job_name = \"cellpose/\"+dataset_name\n",
    "out_dir = os.path.join(tmp_dir, job_name)\n",
    "\n",
    "node_setting = \"\"\n",
    "node_setting = node_setting+f\" --job-name={job_name}\"\n",
    "node_setting = node_setting+\" --time=2-00:00\"\n",
    "node_setting = node_setting+\" --nodes=1\"\n",
    "node_setting = node_setting+hpc_gpu\n",
    "node_setting = node_setting+\" --gres=gpu:gtx1080ti:1\"\n",
    "node_setting = node_setting+\" --mem=64000\"\n",
    "node_setting = node_setting+\" --ntasks-per-node=1\"\n",
    "node_setting = node_setting+f\" --output={out_dir}/'slurm-%A_%a.out'\"\n",
    "node_setting = node_setting[1:]\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "submit_array(tmp_dir, command_list, node_setting, job_name, repo_path=None,conda_path=conda_path,model='cellpose')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-round",
   "metadata": {},
   "source": [
    "## Normalise predictions to 8bit (cellprofiler requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in folders:     \n",
    "    output_dir = f'{gsk_broad_path}/8bit_images_16bit_masks/'\n",
    "    if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)               \n",
    "    samples = sorted([p for p in Path(f'{gsk_broad_path}/').glob(f'**/*syn*.tif*')])\n",
    "    masks = sorted([p for p in Path(f'{gsk_broad_path}/').glob(f'**/*mask*.tif*')])\n",
    "    print(i,len(samples),len(masks))\n",
    "    for index,f in enumerate(samples):\n",
    "        tmp = imread(f)\n",
    "        tmp_8bit = normalize(tmp)\n",
    "        final_name = str(f).split('/')[-1]\n",
    "        final_path = f'{output_dir}/{final_name}'\n",
    "        imsave(final_path,tmp_8bit.astype(np.uint8),imagej=True)\n",
    "        mask = imread(masks[index])\n",
    "        mask = center_crop(mask)\n",
    "        mask_name = str(masks[index]).split('/')[-1]\n",
    "        final_path = f'{output_dir}/{mask_name}'\n",
    "        imsave(final_path,mask,imagej=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942dc7c",
   "metadata": {},
   "source": [
    "## CellProfiler - run pipeline and save features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate input folder and build cellpose command for cell and nuclei segmentation\n",
    "\n",
    "for f in list(folders):\n",
    "    command_list = []\n",
    "    path = os.path.join(\n",
    "        gsk_broad_path,\n",
    "        f,)\n",
    "    ### Run in via SLURM\n",
    "    command_list = []\n",
    "    cp_path = os.path.join(\n",
    "        './cellprofiler/...cpproj')\n",
    "    input_path = os.path.join(\n",
    "        f'{gsk_broad_path}/8bit_images_16bit_masks/'\n",
    "    )\n",
    "\n",
    "    conda_path = '/path/to/cellprofiler/conda/env/'\n",
    "    dataset_name = f'GSK-Broad: {f}' \n",
    "    command = f'cellprofiler -c -r -p {cp_path} -i {input_path} -o {input_path}'\n",
    "    command_list.append(command)\n",
    "    \n",
    "# Job array settings\n",
    "job_name = dataset_name\n",
    "out_dir = os.path.join(tmp_dir, job_name)\n",
    "node_setting = \"\"\n",
    "node_setting = node_setting+f\" --job-name={job_name}\"\n",
    "node_setting = node_setting+\" --time=2-00:00\"\n",
    "node_setting = node_setting+\" --nodes=1\"\n",
    "node_setting = node_setting+hpc_gpu\n",
    "node_setting = node_setting+\" --gres=gpu:gtx1080ti:1\"\n",
    "node_setting = node_setting+\" --mem=64000\"\n",
    "node_setting = node_setting+\" --ntasks-per-node=1\"\n",
    "node_setting = node_setting+\" --export=NONE\"\n",
    "node_setting = node_setting+f\" --output={out_dir}/'slurm-%A_%a.out'\"\n",
    "node_setting = node_setting[1:]\n",
    "\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "submit_sbatch(tmp_dir, command_list, node_setting, job_name, repo_path=None,conda_path=None, model='cellprofiler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-french",
   "metadata": {},
   "source": [
    "## End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cell_profiler_py3.8",
   "language": "python",
   "name": "cell_profiler_py3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
